---
layout: post
title:  OS CheatSheet
categories: Interview
---
# Virtualization
Program execution flow:
CPU fetches information from memory -> decode -> execute

The reason why virtualization is to make the system easier to use, OS can virtualize the hardware resources.

## Process
Process is an abstraction of a **running program** created by the OS. The program itself is a bunch of instructions(and maybe some static data), waiting to spring into action. It is the operating system that gets them running.

To understand what constitutes a process, we thus have to understand its machine state: what a program can read or update when it is running. At any given time, what parts of the machine are important to the execution of this program? One obvious component of machine state that comprises a process is its memory. Instructions lie in memory; the data that the running program reads and writes sits in memory as well. Thus the memory that the process can address (called its address space) is part of the process.

Also part of the process’s machine state are registers; many instructions explicitly read or update registers and thus clearly they are important to the execution of the process.

So process is the resource allocation unit that the OS abstracts so that we can run multiple programs in a same machine easier. The OS using **time sharing** to make many processes to virtualize the CPU resources.

![result](https://github.com/770120041/myblog/blob/master/images/2019-4/MemoryAndCode.png?raw=true)

### Creation of process
1. Load code and static data(initialized values) into memory
2. Allocate memory for the run-time stack and some space for the heap of the program
(As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process.)

a process has different states: running, ready and blocked

### PCB
A process list contains information about all processes in the system. Each entry is found in what is sometimes called a process control block (PCB), which is really just a structure that contains
information about a specific process.

### APIS
1. fork() creates a new process by duplicating the calling process.  The
        new process is referred to as the child process.  The calling process
        is referred to as the parent process.

2. wait() can help to make this determinestic
```c
int main(int argc, char *argv[]) {
    printf("hello world (pid:%d)\n", (int) getpid());
    int rc = fork();
    if (rc < 0) { // fork failed; exit
        fprintf(stderr, "fork failed\n");
        exit(1);
    } else if (rc == 0) { // child (new process)
        printf("hello, I am child (pid:%d)\n", (int) getpid());
    } else { // parent goes down this path (main)
        int rc_wait = wait(NULL);
        printf("hello, I am parent of %d (rc_wait:%d) (pid:%d)\n",
         rc, rc_wait, (int) getpid());
    }
    return 0;
    }
```
3. exec()
In computing, exec is a functionality of an operating system that runs an executable file in the context of an already existing process, replacing the previous executable. This act is also referred to as an overlay. It is especially important in Unix-like systems, although other operating systems implement it as well. Since a new process is not created, the original process identifier (PID) does not change, but the machine code, data, heap, and stack of the process are replaced by those of the new program.

The reason why using this is easier to implement shell.

4. Sending signals to a process
like Kill, Ctri-C, Ctrl-Z signal() system call

## How to time-sharing the CPU with control
Limited-Direct-Execution

Direct execution has the obvious advantage of being fast; the program runs natively on the hardware CPU and thus executes as quickly as one would expect. 

1. How program run restricted operations?
the approach we take is to introduce a new processor mode, known as **user mode**; code that runs in user mode is restricted in what it can do.

In contrast to user mode is kernel mode, which the operating system(or kernel) runs in. In this mode code that runs can do what it likes, including privileged operations such as issuing I/O requests and executing

To users run restricted operations, virtually all modern hardware provides the ability for user programs to perform a system call. To execute a system call, a program must execute a special trap instruction. This instruction simultaneously jumps into the kernel and raises the
privilege level to kernel mode; once in the kernel, the system can now perform whatever privileged operations are needed (if allowed), and thus do the required work for the calling process. When finished, the OS calls a special return-from-trap instruction, which, as you might expect, returns
into the calling user program while simultaneously reducing the privilege level back to user mode.

Before executing the trap, the processor will push the program counter, flags, and a few other registers onto a per-process kernel stack.

2. how trap know which code to run inside the OS
We shouldn't CPU to execute any program when received trap instruction, The kernel does so by setting up a trap table at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be. One of the first things the OS thus does is to tell the hardware what code to run when certain exceptional events occur

![LDE](https://github.com/770120041/myblog/blob/master/images/2019-4/LDE.png?raw=true)

3. How to switch between processes
Regain control: wait for yield or use a timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a pre-configured interrupt handler in the OS runs.
![LDE](https://github.com/770120041/myblog/blob/master/images/2019-4/LDEwithTimer.png?raw=true)

## Scheduling(1)
metrics: 
1. turn around time = time compelted - time arrived
2. response time = time first run - time arrived

FIFO

SJF: shorted job first

STCF:shortest time-to-complete first

round robin

## Scheduling(2): Multi-level feedback queue
• Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t).
• Rule 2: If Priority(A) = Priority(B), A & B run in round-robin fashion using the time slice (quantum length) of the given queue.
• Rule 3: When a job enters the system, it is placed at the highest
priority (the topmost queue).
• Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is
reduced (i.e., it moves down one queue).
• Rule 5: After some time period S, move all the jobs in the system to the topmost queue.(When a program is not using up the time)

PS:
* Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue).
* Rule 4b: If a job gives up the CPU before the time slice is up, it stays
at the same priority level. 

IF a program fool the system? ->  Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is
reduced (i.e., it moves down one queue)

## Scheduling(3): Proportional share(lottery scheduling)
random: can be fase, can avoid cornor cases, can be lightweighted 

## Scheduling(4): Multi-CPU
space locality, cache locality, time locality, So be better run same job in same core   

## Isolation and protection :The Abstraction: Address Spaces
![result](https://github.com/770120041/myblog/blob/master/images/2019-4/memoryModel.png?raw=true)



How to share memory for different processes?One way to implement time sharing would be to run one process for a short while, giving it full access to all memory (Figure 13.1), then stop it, save all of its state to some kind of disk (including all of physical memory), load some other process’s state, run it for a while, and thus implement some kind of crude sharing of the machine. Thus, what
we’d rather do is leave processes in memory while switching between them, allowing the OS to implement time sharing efficiently 

![result](https://github.com/770120041/myblog/blob/master/images/2019-4/processShareMemory.png?raw=true)

But the method above don't provide protection, doing so requires the OS to create an easy to use abstraction of physical memory. We call this abstraction the address space, and it is the running program’s view of memory in the system.

The address space of a process contains all of the memory state of the running program. For example, the code of the program (the instructions) have to live in memory somewhere, and thus they are in the address space. The program, while it is running, uses a stack to keep track of where it is in the function call chain as well as to allocate local variables and pass parameters and return values to and from routines. Finally, the heap is used for dynamically-allocated, user-managed memory, such as that you might receive from a call to malloc() in C or new in an objectoriented language such as C++ or Java. Of course, there are other things in there too (e.g., statically-initialized variables), but for now let us just assume those three components: **code, stack, and heap**

Address space is the running process's view of the memory(an illusion)



![result](https://github.com/770120041/myblog/blob/master/images/2019-4/addressSpace.png?raw=true)

## Address translation: Implementing protection
Using a base register and a limit register(MMU memory management unit). The OS run in kernel mode, and have aceess in the entire machine, while user program run in user mode, and it has limited access. OS set this by a single bit. Modifying base registers are in kernel mode
![result](https://github.com/770120041/myblog/blob/master/images/2019-4/programWorkFlow.png?raw=true)

## Segmentation
Flaws of simple base+limit register:although the space between the stack and heap is not being used by the process, it is still taking up physical memory when we relocate the entire address space somewhere in physical memory; thus, the simple approach of using a base and bounds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn’t fit into memory; thus, base and bounds is not as flexible as we would like. 

Solution:Segmentation, instead of having just one base and bounds pair in our MMU, why not have a base and bounds pair per logical segment of the address space? 
![result](https://github.com/770120041/myblog/blob/master/images/2019-4segmentation.png?raw=true)

What if we tried to refer to an illegal address? Segmentation fault.
![result](https://github.com/770120041/myblog/blob/master/images/segmentsAndOffset.png?raw=true)


The general problem that arises is that physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments, or to grow existing ones. We call this problem external fragmentation [R69]; see Figure 16.6 (left).One solution to this problem would be to compact physical memory  by rearranging the existing segments.  A simpler approach might instead be to use a free-list management algorithm that tries to keep large extents of memory available for allocation. There are literally hundreds of approaches that people have taken, including classic algorithms like best-fit (which keeps a list of free spaces and returns the one closest in size that satisfies the desired allocation to the requester), worst-fit, first-fit, and more complex schemes like buddy algorithm [K68]. An excellent survey by Wilson et al. is a good place to start if you want to learn more about such algorithms [W+95], or you can wait until we cover some of the basics in a later chapter. Unfortunately, though, no matter how smart the algorithm, external fragmentation will still exist; thus, a good algorithm simply attempts to minimize it.

## Paging(1)
Chop up space into fixed-sized pieces. In virtual memory, we call this idea paging), we divide it into fixed-sized units, each of which we call a page. Correspondingly, we view physical memory as an array of fixed-sized slots called page frame.

To record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a per-process data structure known as a page table. The major role of the page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides.

![result](https://github.com/770120041/myblog/blob/master/images/pageTable.png?raw=true)

 page table. In general, a page table stores virtual-to-physical address translations thus letting the system know where each page of an address space actually resides in physical memory. Because each address space requires such translations, in general there is one page table per process in the system. The exact structure of the page table is either determined by the hardware (older systems) or can be more flexibly managed by the OS (modern systems). 

![result](https://github.com/770120041/myblog/blob/master/images/PTE.png?raw=true)

## Paging: Faster Translations (TLBs)
a translation-lookaside buffer, or TLB [CG68, C95]. A TLB is part of the chip’s memory-management unit (MMU), and is simply a hardware cache of popular virtual-to-physical address translations;

## Paging + segmentation
![result](https://github.com/770120041/myblog/blob/master/images/pageWithSegmentation.png?raw=true)

## Swapping
Swap memory to hard disk

Page fault: page not in memory

Page in: swap in a page to memory

Page out: swap to hard disk

<hr>

# Concurrency
## Thread
Thread gives a program more than one point of execution. They share the address space and can access the same data.(More specifically, they share the code segment, data segment, heap segment but it has its own stack and registers). Using thread control block to save its states for context switching

Why use threads: parallelism, handling blocking operations,like IO from network. 

Problem: access the critical area, uncontrolled scheduling

# Persitence